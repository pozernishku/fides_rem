{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import csv\n",
    "import warcat.model\n",
    "import argparse\n",
    "# import logging\n",
    "import os\n",
    "import re\n",
    "from operator import itemgetter, attrgetter\n",
    "\n",
    "regex_html_char_num = re.compile(r\"&#?[0-9a-zA-Z]*;\", re.MULTILINE)\n",
    "html_amp_dash_dict = {'&amp;':'&','&ndash;':'–','&mdash;':'—','&horbar;':'―','&minus;':'−',\n",
    "                     '&hyphen;':'‐','&dash;':'‐','&HorizontalLine;':'─','&hybull;':'⁃',\n",
    "                     '&#45;':'-','&#x2D;':'-','&#X2D;':'-','&#x2d;':'-','&#X2d;':'-',\n",
    "                     '&#8211;':'–','&#x2013;':'–','&#X2013;':'–','&#8212;':'—','&#x2014;':'—',\n",
    "                     '&#X2014;':'—','&#8722;':'−','&#x2212':'−','&#X2212':'−'}\n",
    "\n",
    "regex_non_relevant_symb = re.compile(r\"(?<=(\\w))[^\\s\\w]+(?=(\\w))\", re.MULTILINE)\n",
    "regex_remaining_non_relevant_symb = re.compile(r\"[^\\s\\w]+\", re.MULTILINE)\n",
    "regex_alone_digits = re.compile(r\"(?<=\\s)[\\d]+(?=\\s)\", re.MULTILINE)\n",
    "regex_dot_capital = re.compile(r\"[.](\\w)\", re.MULTILINE)\n",
    "regex_white_space = re.compile(r\"\\s+\", re.MULTILINE)\n",
    "\n",
    "regex_email = re.compile(r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b\", re.MULTILINE | re.IGNORECASE)\n",
    "regex_hashtag = re.compile(r\"(?<!&)#\\w+\\b(?!;)\")\n",
    "regex_www = re.compile(r\"\\b(?:https?://|www\\.)+\\S{3,}\", re.MULTILINE | re.IGNORECASE)\n",
    "\n",
    "dashes_set = {'–','—','―','−','‐','─','⁃','-'}\n",
    "period = '.'\n",
    "ampersand = '&'\n",
    "apostrophe_set = {\"'\",'‵',\"’\",'‘','´','`','′'}\n",
    "digits_set = {'0','1','2','3','4','5','6','7','8','9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "пожалуй, еще нужно добавить фильтр на длину слова. Скажем, слова, более 30 символов - не обрабатывать.\n",
    "- точки между буквами не обрабатываются за искл. сочетание \"точка+одиночная заглавная буква\" разделяются пробелом;\n",
    "'''\n",
    "def remove_long_words(text, word_length=30):\n",
    "    if text is None:\n",
    "        return ''\n",
    "    \n",
    "    a = 0\n",
    "    s = ''\n",
    "    \n",
    "    for i, c in enumerate(text):\n",
    "        if c == ' ':\n",
    "            if i - a <= word_length:\n",
    "                s += final_cuts(text[a:i+1])\n",
    "                a = i+1\n",
    "            else:\n",
    "                z = final_cuts(text[a:i+1]).strip()\n",
    "                if len(z) <= word_length:\n",
    "                    s += z + ' '\n",
    "                a = i+1\n",
    "    else:\n",
    "        if c == ' ':\n",
    "            return s\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        if i - a <= word_length:\n",
    "            s += final_cuts(text[a:i+1])\n",
    "        else:\n",
    "            z = final_cuts(text[a:i+1]).strip()\n",
    "            if len(z) <= word_length:\n",
    "                s += z\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "- отдельно стоящие небуквенные и нецифровые последовательности удалять; это касается и тире, апострофа и амперсанда:\n",
    "Ano Novo – & Australia Day – Good Friday – Easter Saturday - Easter Monday\n",
    "g = 'страница от 10.01.2018. Оригинал '\n",
    "'''\n",
    "def final_cuts(word):\n",
    "    n = len(word)\n",
    "    \n",
    "    if n == 0:\n",
    "        return word\n",
    "    elif n == 1:\n",
    "        if (word in dashes_set or word == period or word == ampersand \n",
    "            or word in apostrophe_set or word == ' ' or word in digits_set):\n",
    "            return ' '\n",
    "        else:\n",
    "            return word\n",
    "    elif n >= 2:\n",
    "        front = word[0]\n",
    "        tail = word[-1]\n",
    "        pre_tail = word[-2:-1]\n",
    "        pre_check_digit = word[-3:-2]\n",
    "        \n",
    "        if (front in dashes_set or front == period or front == ampersand \n",
    "            or front in apostrophe_set):\n",
    "            word = ' ' + word[1:]\n",
    "        \n",
    "        if (tail in dashes_set or tail == period or tail == ampersand \n",
    "            or tail in apostrophe_set):\n",
    "            word = word[:-1] + ' '\n",
    "            \n",
    "        if ((pre_tail in dashes_set or pre_tail == period or pre_tail == ampersand \n",
    "            or pre_tail in apostrophe_set) and tail == ' '):\n",
    "            word = word[:-2] + ' ' + word[-1:]\n",
    "            \n",
    "        if (pre_check_digit in digits_set or pre_tail in digits_set or tail in digits_set or front in digits_set):\n",
    "            word = clean_alone_digits(' ' + word + ' ')\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def html_repl_func(match):\n",
    "    if match.group() in html_amp_dash_dict:\n",
    "        return html_amp_dash_dict[match.group()]\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "-удалить все хтмл-символы, кроме дефиса и амперсанда (их заменять на символ);\n",
    "'''\n",
    "def clean_html_char_num(text):\n",
    "    match = regex_html_char_num.search(text)\n",
    "    if match:\n",
    "        return regex_html_char_num.sub(html_repl_func, text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def non_relevant_repl_func(match):\n",
    "    if len(match.group(0)) > 1:\n",
    "        return ' '\n",
    "    elif ((match.group(0) in dashes_set \n",
    "           or match.group(0) in apostrophe_set \n",
    "           or match.group(0) == period \n",
    "           or match.group(0) == ampersand)\n",
    "          and (match.group(1).isalpha() and match.group(2).isalpha())):\n",
    "        return match.group(0)\n",
    "    elif ((match.group(0) in dashes_set)\n",
    "          and ( (match.group(1).isalpha() and match.group(2) in digits_set)\n",
    "             or (match.group(2).isalpha() and match.group(1) in digits_set))):\n",
    "        return match.group(0)\n",
    "    else:\n",
    "        if match.group(1).isalpha() and match.group(2).isalpha():\n",
    "            return ' '\n",
    "        elif ((match.group(1).isalpha() and match.group(2) in digits_set) \n",
    "             or (match.group(2).isalpha() and match.group(1) in digits_set)\n",
    "             or (match.group(1) in digits_set and match.group(2) in digits_set)):\n",
    "            return ' '\n",
    "        else:\n",
    "            return match.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def dot_capital_repl_func(match):\n",
    "    if match.group(1).isupper():\n",
    "        return ' ' + match.group(1)\n",
    "    else:\n",
    "        return match.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "-все небуквенные и нецифровые символы между буквами заменяются на пробелы за исключением:\n",
    "тире (все виды), точки и апострофа (их тоже несколько) между буквами не обрабатываются;\n",
    "тире (все виды) между сочетаниями буква-цифра (и наоборот) не обрабатываются;\n",
    "'''\n",
    "def clean_non_relevant_symb(text):\n",
    "    match = regex_non_relevant_symb.search(text)\n",
    "    if match:\n",
    "        return regex_non_relevant_symb.sub(non_relevant_repl_func, text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "цифровые последовательности, не содержащие букв, удалятся;\n",
    "# цифры в перемешку с небуквенными символами остаются: A-077-B\n",
    "'''\n",
    "def clean_alone_digits(text):\n",
    "    match = regex_alone_digits.search(text)\n",
    "    if match:\n",
    "        return regex_alone_digits.sub('', text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "сочетание \"точка+одиночная заглавная буква\" разделяются пробелом;\n",
    "name: DUC. Enjoy - все точки также удаляются, после обаботки: сочетание \"точка+одиночная заглавная буква\" разделяются пробелом;\n",
    "(Тут заменяются на пробел только те точки, которые стоят перед заглавной буквой)\n",
    "'''\n",
    "def separate_dot_capital(text):\n",
    "    match = regex_dot_capital.search(text)\n",
    "    if match:\n",
    "        return regex_dot_capital.sub(dot_capital_repl_func, text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "name: DUC. Enjoy - все точки также удаляются, после обаботки: сочетание \"точка+одиночная заглавная буква\" разделяются пробелом; \n",
    "удалено условие: тут удаляются все точки [^\\s\\w]+\n",
    "'''\n",
    "def remaining_non_relevant_repl_func(match):\n",
    "    if len(match.group(0)) > 1:\n",
    "        return ' '\n",
    "    elif (match.group(0) == ampersand \n",
    "          or match.group(0) in dashes_set \n",
    "          or match.group(0) == period \n",
    "          or match.group(0) in apostrophe_set):\n",
    "        \n",
    "        if match.start() == 0 or match.start() == match.endpos-1:\n",
    "            return ' '\n",
    "        \n",
    "        return match.group(0)\n",
    "    else:\n",
    "        return ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# НЕ из перечисленных: амперсанд (&), тире(все виды), точка (.), апостраф(все виды)\n",
    "'''\n",
    "идея же в том, чтобы избавиться от небуквенных последовательностей, а также \"помочь\" токенизации при построении частотного распределения. \n",
    "\n",
    "например, \n",
    "stuff, here - тут запятой не должно быть;\n",
    "Flights: FLAGRANTS - здесь не нужно двоеточие;\n",
    "Germany-c60-199?) - здесь скобки и вопр. знака;\n",
    "'''\n",
    "def clean_remaining_non_relevant_symb(text):\n",
    "    match = regex_remaining_non_relevant_symb.search(text)\n",
    "    if match:\n",
    "        return regex_remaining_non_relevant_symb.sub(remaining_non_relevant_repl_func, text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "кстати, выделенные url, e-mail, hash-tag могут также пригодиться для оценки, \n",
    "поэтому есть смысл их не просто удалять, а и по ним строить частотное распределение \n",
    "(например, их кол-во и распределение могут показывать \"связность\" ресурса).\n",
    "'''\n",
    "def clean_from_emails(text):\n",
    "    match = regex_email.search(text)\n",
    "    if match:\n",
    "        return regex_email.sub(' ', text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_from_www(text):\n",
    "    match = regex_www.search(text)\n",
    "    if match:\n",
    "        return regex_www.sub(' ', text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def clean_from_hashtag(text):\n",
    "    match = regex_hashtag.search(text)\n",
    "    if match:\n",
    "        return regex_hashtag.sub(' ', text)\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def strip_port(res_url):\n",
    "    res_len = len(res_url)\n",
    "    for d in range(1, 7 if res_len > 5 else res_len+1):\n",
    "        if res_url[-d] == ':':\n",
    "            return res_url[:-d]\n",
    "    else:\n",
    "        return res_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# h or w\n",
    "# if h then stop on third / or on the end of the string (if no more /) https://test.com   www.test.com/\n",
    "# if w then stop on first / or on the end of the string (if no more /)\n",
    "def strip_urls(url):\n",
    "    cnt = 0\n",
    "    if len(url) == 0: return url\n",
    "    first = url[0]\n",
    "    \n",
    "    for i, c in enumerate(url):\n",
    "        if c == '/':\n",
    "            cnt += 1\n",
    "            if (first == 'h' and cnt == 3) or (first == 'w' and cnt == 1):\n",
    "                return strip_port(url[:i]).strip('\\\\')\n",
    "    else:\n",
    "        return strip_port(url).strip('\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Clean text according to http://redmine-ots.co.spb.ru/issues/7415\n",
    "'''\n",
    "def clean_text(text):\n",
    "    return remove_long_words(\n",
    "                                regex_white_space.sub(' ', \n",
    "                                     clean_alone_digits(\n",
    "                                        clean_remaining_non_relevant_symb(\n",
    "                                            separate_dot_capital(\n",
    "                                                clean_non_relevant_symb(\n",
    "                                                    clean_from_hashtag(\n",
    "                                                        clean_from_www(\n",
    "                                                            clean_html_char_num(\n",
    "                                                                clean_from_emails(text))))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Сокращение хвостов происходит по проценту от общей суммы слов (90-95%), это значит: \n",
    "суммируются частоты всех слов (=100%), список слов ранжируется, например, по убыванию, \n",
    "и 5-10% «хвост» низкочастотного распределения отрезается (это почти 100% шум: ошибки, описки, прочий хлам). \n",
    "При этом общий объем частотного словаря уменьшится более, чем в два раза.\n",
    "\n",
    "т.е. идея учитывать \"плотность\" распределения, а не просто обрезать по кол-ву.\n",
    "тогда мы вынем нужные слова, а мусор выкинем.\n",
    "\n",
    "и еще один нюанс: часто порог попадает на группу слов с одинаковой частотой, особенно, если объем текста большой,\n",
    "так вот, хорошо бы либо всю эту группу либо отбрасывать, либо включать;\n",
    "\n",
    "стоит добавить ещё один параметр (?) strip_ones=1\n",
    "- думаю, да, пригодится.\n",
    "'''\n",
    "def fr_dist_with_domain(text, ref, slice_percent=90, short_tail=1, strip_ones=1, lang_percent=80):\n",
    "    words_list = text.lower().split()\n",
    "    domain = strip_urls(ref).lower()\n",
    "        \n",
    "    di = dict()\n",
    "    \n",
    "    for w in words_list:\n",
    "        di[w] = di.get(w, 0) + 1\n",
    "    \n",
    "    items = sorted(di.items(), key=itemgetter(1), reverse=True)\n",
    "    result_items = []\n",
    "            \n",
    "    cnt_all = len(words_list)\n",
    "    cnt_dist = len(items)\n",
    "    \n",
    "    if cnt_all == 0:\n",
    "        return [(' ', 0, domain),]\n",
    "    elif cnt_all == 1:\n",
    "        return drop_non_latin_rus([(words_list[0], 1, domain),], domain, lang_percent)\n",
    "    \n",
    "    if cnt_dist == 1:\n",
    "        return drop_non_latin_rus([(items[0] + (domain,)),], domain, lang_percent)\n",
    "    \n",
    "    cur_prc = 0\n",
    "    cnt_fr_words = 0\n",
    "    first_one_idx = None\n",
    "    cnt_border = 25 # Change to 25!\n",
    "    \n",
    "    for i in range(cnt_dist):\n",
    "        cur_prc += 100 / (cnt_all/items[i][1])\n",
    "        prev_cnt = 0 if i == 0 else items[i-1][1]\n",
    "        cnt_fr_words += 1 if items[i][1] > 1 else 0\n",
    "        first_one_idx = i if first_one_idx is None and items[i][1] == 1 else first_one_idx\n",
    "        \n",
    "#         print(items[i][0], items[i][1], cur_prc, prev_cnt, cnt_fr_words, first_one_idx)\n",
    "        \n",
    "        if short_tail:\n",
    "            if cur_prc > slice_percent:\n",
    "                if strip_ones == 1 and cnt_fr_words > cnt_border:\n",
    "                    return strip_ones_func(drop_non_latin_rus(result_items, domain, lang_percent), \n",
    "                                           first_one_idx)\n",
    "                else:\n",
    "                    return drop_non_latin_rus(result_items, domain, \n",
    "                                              lang_percent) if result_items else [(' ', 0, domain),]\n",
    "            else:\n",
    "                result_items.append(items[i] + (domain,))\n",
    "        else:\n",
    "            if cur_prc > slice_percent:\n",
    "                if items[i][1] == prev_cnt:\n",
    "                    result_items.append(items[i] + (domain,))\n",
    "                else:\n",
    "                    if strip_ones == 1 and cnt_fr_words > cnt_border:\n",
    "                        return strip_ones_func(drop_non_latin_rus(result_items, domain, lang_percent), \n",
    "                                               first_one_idx)\n",
    "                    else:\n",
    "                        return drop_non_latin_rus(result_items, domain, \n",
    "                                                  lang_percent) if result_items else [(' ', 0, domain),]\n",
    "            else:\n",
    "                result_items.append(items[i] + (domain,))\n",
    "    else:\n",
    "        if strip_ones == 1 and cnt_fr_words > cnt_border:\n",
    "            return strip_ones_func(drop_non_latin_rus(result_items, domain, lang_percent), first_one_idx)\n",
    "        else:\n",
    "            return drop_non_latin_rus(result_items, domain, lang_percent) if result_items else [(' ', 0, domain),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "оставить языки которые используют:\n",
    "- латиницу\n",
    "- русский\n",
    "\n",
    "Filter used in fr_dist_with_domain which drops out frequency distributions \n",
    "with low language percent of cyrillic or latin chars.\n",
    "'''\n",
    "\n",
    "def drop_non_latin_rus(items, domain, lang_percent=80):\n",
    "    symb_count_all = 0\n",
    "    symb_count_fit = 0\n",
    "    \n",
    "    if not items:\n",
    "        return [(' ', 0, domain),] # adds empty record with specified domain \n",
    "    else:\n",
    "        for item in items:\n",
    "            word = item[0]\n",
    "            word_frequency = item[1]\n",
    "            n = len(word)\n",
    "            N = 0\n",
    "            for c in word:\n",
    "                x = ord(c)\n",
    "                if ((x >= 0x0000 and x <= 0x02AF)    # below are blocks of Latin script in Unicode\n",
    "                    or (x >= 0x1D00 and x <= 0x1DBF) # https://en.wikipedia.org/wiki/Latin_script_in_Unicode\n",
    "                    or (x >= 0x1E00 and x <= 0x1EFF)\n",
    "                    or (x >= 0x2070 and x <= 0x209F)\n",
    "                    or (x >= 0x2100 and x <= 0x218F)\n",
    "                    or (x >= 0x2C60 and x <= 0x2C7F)\n",
    "                    or (x >= 0xA720 and x <= 0xA7FF)\n",
    "                    or (x >= 0xAB30 and x <= 0xAB6F)\n",
    "                    or (x >= 0xFB00 and x <= 0xFB06)\n",
    "                    or (x >= 0xFF00 and x <= 0xFF64)\n",
    "                    or (x >= 0x2460 and x <= 0x24FF) # https://en.wikipedia.org/wiki/Enclosed_Alphanumerics\n",
    "                    or (x >= 0x3248 and x <= 0x325F) # https://en.wikipedia.org/wiki/Enclosed_CJK_Letters_and_Months\n",
    "                    or (x >= 0x32B1 and x <= 0x32BF) # continue\n",
    "                    or (x >= 0x1D400 and x <= 0x1D7FF)#https://en.wikipedia.org/wiki/Mathematical_Alphanumeric_Symbols\n",
    "                    or (x >= 0x1F100 and x <= 0x1F1FF)\n",
    "                    or (x >= 0xA4D0 and x <= 0xA4FF) # https://en.wikipedia.org/wiki/Lisu_(Unicode_block)\n",
    "                    \n",
    "                    or (x >= 0x0400 and x <= 0x052F) # below are blocks of Cyrillic script in Unicode\n",
    "                    or (x >= 0x2DE0 and x <= 0x2DFF) # https://en.wikipedia.org/wiki/Cyrillic_script_in_Unicode\n",
    "                    or (x >= 0xA640 and x <= 0xA69F)\n",
    "                    or (x >= 0x1C80 and x <= 0x1C8F)\n",
    "                    \n",
    "                    or (chr(x) in dashes_set)\n",
    "                    or (chr(x) == period)\n",
    "                    or (chr(x) == ampersand)\n",
    "                    or (chr(x) in apostrophe_set)\n",
    "                    or (chr(x) in digits_set)\n",
    "                   ):\n",
    "                    N += 1\n",
    "                else:\n",
    "                    N += 0\n",
    "#                 print(c, n, N, word_frequency)\n",
    "            else:\n",
    "                symb_count_all += n * word_frequency\n",
    "                symb_count_fit += N * word_frequency\n",
    "            \n",
    "#             print(item, n, N, word_frequency, symb_count_all, symb_count_fit)\n",
    "        else:\n",
    "            prc_fit = symb_count_fit / symb_count_all * 100\n",
    "            \n",
    "            if prc_fit >= lang_percent:\n",
    "#                 print('>>>>>>> ADDED!', domain, lang_percent)\n",
    "#                 print(symb_count_all, symb_count_fit)\n",
    "#                 print(items)\n",
    "#                 print()\n",
    "                return items\n",
    "            else:\n",
    "#                 print('>>>>>>> DROPED!', domain, lang_percent)\n",
    "#                 print(symb_count_all, symb_count_fit)\n",
    "#                 print(items)\n",
    "#                 print()\n",
    "                return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def strip_ones_func(items, idx):\n",
    "    if items and idx is not None:\n",
    "        return items[:idx]\n",
    "    elif items and idx is None:\n",
    "        return items\n",
    "    elif not items:\n",
    "        return items\n",
    "#         return [(' ', 0, dmn),] # removed parameter dmn\n",
    "    else:\n",
    "        return items[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "wet_list also accepts compressed files *.warc.wet.gz\n",
    "Процент обрезания задавать параметрически, чтобы постом можно было подобрать оптимальный.\n",
    "'''\n",
    "def clean_tokenize_frqdis_wet_files(wet_list=None, done_list_file='wet.paths.done', \n",
    "                                    slice_percent=90, short_tail=1, strip_ones=1, lang_percent=80):\n",
    "    if not wet_list:\n",
    "        print('wet_list is not specified')\n",
    "        return\n",
    "    \n",
    "    done_set = set()\n",
    "    \n",
    "    try:\n",
    "        with open(done_list_file, newline='') as f:\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                done_set.add(row.pop())\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return\n",
    "    \n",
    "#     wet_list = wet_list[-2:-1] # one (last 00639) in list (require all list)\n",
    "    wet_list = wet_list[0:3] #!!!!\n",
    "    \n",
    "    for wet_file in wet_list:\n",
    "        # new iteration if wet_file is done earlier\n",
    "        if wet_file[3:] in done_set:\n",
    "            print(wet_file[3:], 'is in', done_list_file, '- skipped.')\n",
    "            continue\n",
    "        \n",
    "        warc = warcat.model.WARC()\n",
    "            \n",
    "        try:\n",
    "            warc.load(wet_file)\n",
    "        except Exception as e:\n",
    "            print('Error in ', wet_file)\n",
    "            with open(os.path.join('./output', wet_file[3:] + '.error'), 'w') as e_f:\n",
    "                e_f.write(str(e))\n",
    "            continue\n",
    "        \n",
    "        pth = os.path.join('./output', wet_file[3:])\n",
    "#         lg = os.path.join('./logs', wet_file[3:]) # logging - continue\n",
    "\n",
    "        os.makedirs(pth, exist_ok=True)\n",
    "#         os.makedirs(lg, exist_ok=True)\n",
    "        \n",
    "        print('File: ', wet_file, 'Records: ', len(warc.records), sep='\\t', end='\\n\\n') # to logs is better\n",
    "        \n",
    "        wet_fr_dist = []\n",
    "        \n",
    "        for i, record in enumerate(warc.records): # sliced here! warc.records[:50]\n",
    "            file_uri = record.header.fields.get('WARC-Target-URI')\n",
    "            print(record.header.fields.list(), 'Num: ', i, sep='\\t', end='\\n\\n')\n",
    "            \n",
    "            if record.warc_type != 'warcinfo':\n",
    "                with record.content_block.get_file() as f:\n",
    "                    text = bytes.decode(f.read())\n",
    "                    \n",
    "                    # а и по ним строить частотное распределение\n",
    "                    emails = ' '.join(regex_email.findall(text))\n",
    "                    sites = ' '.join(map(strip_urls, regex_www.findall(text)))\n",
    "                    hash_tags = ' '.join(regex_hashtag.findall(text))\n",
    "                    \n",
    "                    cleaned_text = clean_text(text) + '  ' + emails + '  ' + sites + '  ' + hash_tags\n",
    "                    wet_fr_dist.extend(fr_dist_with_domain(cleaned_text, file_uri, slice_percent, \n",
    "                                                           short_tail, strip_ones, lang_percent))\n",
    "                    \n",
    "        else: # WET file end loop -- save to csv\n",
    "            file_name_wet_csv = wet_file[3:] + '.csv'\n",
    "            with open(os.path.join(pth, file_name_wet_csv), 'w', newline='') as csv_f:\n",
    "                writer = csv.writer(csv_f, delimiter='\\t')\n",
    "                writer.writerows(wet_fr_dist)\n",
    "            \n",
    "            # Add WET file name to wet.paths.done list\n",
    "            with open(done_list_file, 'a', newline='') as f:\n",
    "                writer = csv.writer(f, delimiter='\\t')\n",
    "                writer.writerows([(wet_file[3:],)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     clean_tokenize_frqdis_wet_files(glob.glob(\"../*.warc.wet*\"), 'wet.paths.done', 90, 1, 1, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('slice_percent', \n",
    "                        help='Slice percent. Used to cut off the trash tail of the frequency distribution.',\n",
    "                        type=int)\n",
    "    parser.add_argument('short_tail',\n",
    "                        help='Short tail. Used to preserve the tail which has the words with equal frequency.',\n",
    "                        type=int)\n",
    "    parser.add_argument('strip_ones',\n",
    "                        help='Strip ones. Used to strip words with freq of one if more than 25 words have freq > 1.',\n",
    "                        type=int)\n",
    "    parser.add_argument('lang_percent',\n",
    "                        help='Language percent. Used to drop out frequency distributions with low language percent of cyrillic or latin chars.',\n",
    "                        type=int)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    clean_tokenize_frqdis_wet_files(glob.glob(\"../*.warc.wet*\"), 'wet.paths.done', args.slice_percent, \n",
    "                                    args.short_tail, args.strip_ones, args.lang_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = fasttext.tokenize('tet I`m. Go with me Mr. John, how are you. you`d like it \\ \\ + f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(fasttext.FastText)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
